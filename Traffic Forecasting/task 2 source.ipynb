{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Way to Prediction Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_shape:  (242391, 4)\n",
      "test_shape:  (103907, 4)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('dataset/train_data.csv', sep=';').dropna()\n",
    "test_data = pd.read_csv('dataset/test_data.csv', sep=';').dropna()\n",
    "\n",
    "train_data.columns = ['1','Time', 'Bytes', 'Packages']\n",
    "test_data.columns = ['1','Time', 'Bytes', 'Packages']\n",
    "print('train_shape: ',train_data.shape)\n",
    "print('test_shape: ',test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отрбрасываем выбросы данных \n",
    "def clearData(dataset):\n",
    "    ind_missing = train_data[train_data['Bytes'] > 1039828].index\n",
    "    while len(ind_missing) != 0:\n",
    "        for index in ind_missing:\n",
    "            mean = (train_data[\"Bytes\"][index - 1] + train_data[\"Bytes\"][index + 1]) / 2\n",
    "            train_data[\"Bytes\"][index] = mean\n",
    "        ind_missing = train_data[train_data['Bytes'] > 1039828].index\n",
    "        \n",
    "    ind_missing = train_data[train_data['Packages'] > 2000].index\n",
    "    while len(ind_missing) != 0:\n",
    "        for index in ind_missing:\n",
    "            mean = (train_data[\"Packages\"][index - 1] + train_data[\"Packages\"][index + 1]) / 2\n",
    "            train_data[\"Packages\"][index] = mean\n",
    "        ind_missing = train_data[train_data['Packages'] > 2000].index\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDelta = 3600\n",
    "LAST_COUNT = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean data from emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-ab4f7163e4e6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclearData\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtest_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclearData\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-0617496278e0>\u001B[0m in \u001B[0;36mclearData\u001B[0;34m(dataset)\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mindex\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mind_missing\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m             \u001B[0mmean\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Packages\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindex\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mtrain_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Packages\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindex\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m             \u001B[0mtrain_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Packages\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmean\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m         \u001B[0mind_missing\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtrain_data\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Packages'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m2000\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.5/site-packages/pandas/core/series.py\u001B[0m in \u001B[0;36m__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m   1036\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1037\u001B[0m         \u001B[0;31m# do the setitem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1038\u001B[0;31m         \u001B[0mcacher_needs_updating\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_is_chained_assignment_possible\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1039\u001B[0m         \u001B[0msetitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1040\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcacher_needs_updating\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m_check_is_chained_assignment_possible\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   3197\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mref\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mref\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_mixed_type\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3198\u001B[0m                 self._check_setitem_copy(stacklevel=4, t='referant',\n\u001B[0;32m-> 3199\u001B[0;31m                                          force=True)\n\u001B[0m\u001B[1;32m   3200\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3201\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_copy\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001B[0m in \u001B[0;36m_check_setitem_copy\u001B[0;34m(self, stacklevel, t, force)\u001B[0m\n\u001B[1;32m   3243\u001B[0m             \u001B[0;31m# the copy weakref\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3244\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3245\u001B[0;31m                 \u001B[0mgc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   3246\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mgc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_referents\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_copy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3247\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_is_copy\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_data = clearData(train_data)\n",
    "test_data = clearData(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка разброса данных байтов\n",
    "train_data.boxplot(column=['Bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"Bytes\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findIndexWhereNextTime(startIndex, data, timeDelta):\n",
    "#     print(startIndex)\n",
    "    startIndex += 1\n",
    "    startTime = data[startIndex][1]\n",
    "    # Чтобы примерно попадать в интервал\n",
    "    offset = 50\n",
    "    for index in range(startIndex, len(data)):\n",
    "        time = data[index][1]\n",
    "        \n",
    "        if (startTime + timeDelta) == time:\n",
    "#             print(\"Delta Time == time\")\n",
    "            return index\n",
    "        \n",
    "        if time > (startTime + timeDelta - offset) and (time < (startTime + timeDelta + offset)):\n",
    "#             print(\"Time in interval between two indexes\")\n",
    "            return index\n",
    "        \n",
    "        if timeDelta < (time - data[index - 1][1]):\n",
    "#             print(\"Time delta Less than Interval\")\n",
    "            return index\n",
    "    \n",
    "    return None\n",
    "        \n",
    "def generateDatasetWithSumElements(dataset):\n",
    "    dataset = dataset.values\n",
    "    newDataset = []\n",
    "    \n",
    "    # current index \n",
    "    currentIndex = 0\n",
    "    time = dataset[currentIndex][1]\n",
    "    findNextIndex = findIndexWhereNextTime(currentIndex, dataset, timeDelta)\n",
    "    \n",
    "    while findNextIndex != None:\n",
    "        newBytes = dataset[currentIndex: findNextIndex][:,2].sum()\n",
    "        newPackages = dataset[currentIndex: findNextIndex][:,3].sum()\n",
    "        newDataset.append([time, newBytes, newPackages])\n",
    "        \n",
    "        currentIndex = findNextIndex\n",
    "        time = dataset[currentIndex][1]\n",
    "        findNextIndex = findIndexWhereNextTime(currentIndex, dataset, timeDelta)\n",
    "#         print(\"Current Index \", currentIndex, \" Next \", findNextIndex)\n",
    "    return newDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.array(generateDatasetWithSumElements(train_data))\n",
    "# избавляемся от первого непонятного всплеска\n",
    "# train_dataset = np.delete(train_dataset,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1)\n",
    "xdata = train_dataset[:,0]\n",
    "ydata = train_dataset[:,1]\n",
    "ax.plot(xdata, ydata)\n",
    "# ax.set_ylim(train_dataset[:,1][1])\n",
    "plt.show(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1)\n",
    "xdata = train_dataset[:,0]\n",
    "ydata = train_dataset[:,2]\n",
    "ax.plot(xdata, ydata)\n",
    "# ax.set_ylim(train_dataset[:,2][1])\n",
    "plt.show(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[:,2][2]\n",
    "# train_dataset = np.delete(train_dataset,1,1)train_dataset[:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = np.delete(train_dataset,0,1)\n",
    "n = len(train_dataset)\n",
    "train_data = pd.DataFrame(train_dataset[0:int(n*0.7)], columns=[ 'Bytes', 'Packages'])\n",
    "val_data = pd.DataFrame(train_dataset[int(n*0.7):], columns=['Bytes', 'Packages'])\n",
    "# train_data = pd.DataFrame(train_dataset[0:int(n*0.7)], columns=[ 'Bytes'])\n",
    "# val_data = pd.DataFrame(train_dataset[int(n*0.7):], columns=['Bytes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# create an abs_scaler object\n",
    "abs_scaler = MaxAbsScaler()\n",
    "\n",
    "# calculate the maximum absolute value for scaling the data using the fit method\n",
    "abs_scaler.fit(train_data)\n",
    "\n",
    "# the maximum absolute values calculated by the fit method\n",
    "abs_scaler.max_abs_\n",
    "# array([4.0e+05, 1.7e+01])\n",
    "\n",
    "# transform the data using the parameters calculated by the fit method (the maximum absolute values)\n",
    "train_scaled_data = abs_scaler.transform(train_data)\n",
    "\n",
    "# store the results in a data frame\n",
    "normalizedTrain = pd.DataFrame(train_scaled_data, columns=train_data.columns)\n",
    "\n",
    "val_scaled_data = abs_scaler.transform(val_data)\n",
    "normalizedVal = pd.DataFrame(val_scaled_data, columns=val_data.columns)\n",
    "\n",
    "# visualize the data frame\n",
    "print(\"train shape =\", normalizedTrain.shape)\n",
    "print(\"val shape =\", normalizedVal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDataset(dataset):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    size = len(dataset) - LAST_COUNT - 1\n",
    "    for index in range(0, size):\n",
    "        x_train.append(list(dataset[index: index + LAST_COUNT]))\n",
    "        y_train.append(dataset[index + LAST_COUNT])\n",
    "    return (np.array(x_train), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generateDataset(normalizedTrain.values)\n",
    "X_val, y_val = generateDataset(normalizedVal.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, trainX, trainY, validData, patience=100, MAX_EPOCHS = 100):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                    patience=patience,\n",
    "                                                    mode='auto')\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.1),\n",
    "                metrics=[tf.keras.metrics.MeanAbsolutePercentageError(),tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    history = model.fit(trainX, trainY,batch_size=24, epochs=MAX_EPOCHS,\n",
    "                      validation_data=validData,\n",
    "                      callbacks=[early_stopping])\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "multi_step_dense = tf.keras.Sequential([\n",
    "    # Shape: (time, features) => (time*features)\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=2)\n",
    "    # Add back the time dimension.\n",
    "    # Shape: (outputs) => (1, outputs)\n",
    "#     tf.keras.layers.Reshape([1, -1]),\n",
    "])\n",
    "\n",
    "\n",
    "history = compile_and_fit(multi_step_dense, X_train, y_train, (X_val, y_val), MAX_EPOCHS= 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test on trained data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = multi_step_dense.predict(X_val)\n",
    "\n",
    "f, ax = plt.subplots(1)\n",
    "# xdata = y_test.values[:,0]\n",
    "ax.plot(y_val[:,0])\n",
    "ax.plot(predicted[:,0])\n",
    "# ax.set_ylim(train_dataset[:,2][1])\n",
    "plt.show(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = multi_step_dense.predict(X_train)\n",
    "\n",
    "f, ax = plt.subplots(1)\n",
    "# xdata = y_test.values[:,0]\n",
    "ax.plot(y_train[:,0])\n",
    "ax.plot(predicted[:,0])\n",
    "# ax.set_ylim(train_dataset[:,2][1])\n",
    "plt.show(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = multi_step_dense.predict(X_test)\n",
    "multi_step_dense.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = multi_step_dense.predict(X_test)\n",
    "multi_step_dense.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(predicted, columns=[ 'Bytes', 'Packages'])\n",
    "y_test = pd.DataFrame(y_test, columns=[ 'Bytes', 'Packages'])\n",
    "\n",
    "# y_test_un = unNormalizedData(y_test,trainMean, trainSTD)\n",
    "# predicted_un = unNormalizedData(predicted, trainMean, trainSTD)\n",
    "# predicted_un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test.values[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((predicted).values[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1)\n",
    "xdata = y_test.values[:,0]\n",
    "ax.plot(xdata)\n",
    "ax.plot(predicted.values[:,0])\n",
    "# ax.set_ylim(train_dataset[:,2][1])\n",
    "plt.show(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_error(actual, predicted):\n",
    "    res = np.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100\n",
    "\n",
    "# predicted_un = predicted_un * -1\n",
    "print(\"MAPE BYTES = \", mean_absolute_percentage_error(y_test.values[:,0], predicted.values[:,0]))\n",
    "print(\"MAPE PACKAGES = \", mean_absolute_percentage_error(y_test.values[:,1], predicted.values[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversed_predicted = abs_scaler.inverse_transform(predicted)\n",
    "inverse_y_test = abs_scaler.inverse_transform(y_test)\n",
    "\n",
    "print(\"Inversed MAPE BYTES = \", mean_absolute_percentage_error(inverse_y_test[:,0], inversed_predicted[:,0]))\n",
    "print(\"Inversed MAPE PACKAGES = \", mean_absolute_percentage_error(inverse_y_test[:,1], inversed_predicted[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1)\n",
    "ax.plot(inverse_y_test[:,0])\n",
    "ax.plot(inversed_predicted[:,0])\n",
    "# ax.set_ylim(train_dataset[:,2][1])\n",
    "plt.show(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_dense.save(\"ouput.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-3ea30826",
   "language": "python",
   "display_name": "PyCharm (NewTrafficForecasting)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}